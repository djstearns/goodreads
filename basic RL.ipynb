{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Nithyashree-2022/A-Hands-on-introduction-to-Reinforcement-Learning-RL-with-Python/blob/main/RL_Basic_implementation_Python.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123ab67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135de14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Environment class\n",
    "class MyEnvironment:\n",
    "\n",
    "  def __init__(self):\n",
    "    #maximum number of steps which the agent can take to gain rewards\n",
    "    self.remaining_steps=20 #assume that the game must be completed within 20 steps\n",
    "\n",
    "  def get_observation(self):\n",
    "    #it can be any number of coordinates.Its considered as 3 here.\n",
    "    #These values -0.0,0.0,0.0 represent some kind of logic that gives info about the environment.These values can be anything.\n",
    "    return [1.0,2.0,1.0]  \n",
    "  \n",
    "  #when agent,performs an action,it should get a reward\n",
    "  #i have set it as 1 for reward,-1 for punishment\n",
    "  def get_actions(self):\n",
    "    return [-1,1]\n",
    "\n",
    "  #if steps are completed,return True because the agent should not move anymore\n",
    "  def check_is_done(self)->bool:\n",
    "    return self.remaining_steps==0\n",
    "\n",
    "  def action(self,int):\n",
    "    if self.check_is_done():\n",
    "      raise Exception(\"Game over\")      \n",
    "    self.remaining_steps-=1  #if steps can still be taken-game not finished=>decrement the remaining number of steps\n",
    "    return random.random()  #here-as this is a simple implementation-just returning a random number\n",
    "#agent implements some policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f413bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myAgent:\n",
    "  def __init__(self):\n",
    "    self.total_rewards=0.0 #initially-agent-no rewards\n",
    "\n",
    "  def step(self,ob:MyEnvironment):\n",
    "    curr_obs=ob.get_observation()\n",
    "    print(curr_obs)\n",
    "    curr_action=ob.get_actions()\n",
    "    print(curr_action)\n",
    "    curr_reward=ob.action(random.choice(curr_action)) \n",
    "    #here,we are randomly picking -1 or 1\n",
    "    #usually,when action() is invoked,implementation to check if the decision of the agent is crt-give positive reward else negative reward\n",
    "    self.total_rewards+=curr_reward\n",
    "    print(\"Total rewards so far= %.3f \"%self.total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e1e31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 0.802 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 1.172 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 2.015 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 2.783 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 2.947 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 3.443 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 4.427 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 4.989 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 5.989 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 6.651 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 6.739 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 7.202 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 8.181 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 8.379 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 8.960 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 9.177 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 9.287 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 9.912 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 10.021 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 11.013 \n",
      "Total reward is 11.013 \n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    obj=MyEnvironment()\n",
    "    agent=myAgent()\n",
    "    step_number=0\n",
    "    while not obj.check_is_done():\n",
    "        step_number+=1\n",
    "        agent.step(obj)\n",
    "    \n",
    "\n",
    "    print(\"Total reward is %.3f \"%agent.total_rewards)\n",
    "    #different o/p everytime we run this code b'coz diff random numbers will be generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
